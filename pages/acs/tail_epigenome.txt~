## Overview

This project is called 'PREDICTD': Parallel Epigenomics Data Imputation with Cloud-based Tensor Decomposition. 

- Epigenome Imputation: Epigenome is all of the machinery associated with the genome: Protein manufacture, gene regulation, and so on.
- The method used here is called Stochastic Gradient Descent Optimization.
- A prior study used for comparatives is called CHROMIMPUTE. The paper for this was published by Ernst and Kellis in 2015 in Nature Biotechnology.
- The researcher for this study is Tim Durham, a graduate student in Bill Noble's genomics lab at the University of Washington.
  - The results improve over the CHROMIMPUTE study for global mean squared error.
  - CHROMIMPUTE does better by comparison on the other metrics; but the methods are still close. 
  - The computation is a form of machine learning (modeling) and is built on the Apache Spark engine
  - The data volume is a three-dimensional tensor with axes: Human cell types, human proteins, and the human genome
    - The cell type axis is 120 or so cell types
    - The protein axis is similarly 150 or so proteins
    - The genome axis is three billion nucleotides
  - The model is trained and one important result is that two of the model components can be locally optimized using only 1% of the genome
  - Therefore the entire genome would take some time... would take 24-48 hours depending on the cluster size
  - Today 1% of the genome requires five hours on 64 cores: That is an X1.16xlarge (more memory than needed; but still cheap per core)
    - optimized data structures to use compressed-format matrices
    - cell type and assay matrix training used 1% of 1% of the data; so that all fit on one machine
    - 500GB required; 1.4TB available
- First write the paper
- Second publish the software and tutorial on GitHub which we will reference
  - Add some interpretability to the code
  - HDInsight or EMR on AWS: Is fine.
  - Tim ran Spark EC2 which bootstraps an EC2 cluster... but you need to be root
    - Spark EC2 is from AmpLab on GitHub.com/amplab/spark-ec2
    - AWS makes it hard for you to log in as root...
  - EMR now permits some spot market access (although the X1.16xlarge may or may not be part of that)
- Third do the entire genome
  - Publish the results "whole genome imputed tracks through the Encyclopedia of DNA Elements 'ENCODE': Hopefully be start of summer
- Tim will turn next to some wet lab work: Studying tissue development in a nematode C.elegans
- The plan is to impute and publish the data tensor for the complete genome-axis. 
  - There are three obstacles in the way of imputing the whole genome right now. 
    - The first (and much bigger) issue is that this will take more time and money than I currently have
    - It's proving harder than I thought it would to get Spark to swallow the full data set, so I will have to do 
      some additional software engineering to process the full genome in batches. 
    - The third reason that we have de-prioritized training on the whole genome is that we've found that the model performs 
      essentially as well on 0.01% of the data as it does on 1% of the data, so we don't expect to get much of a performance 
      boost by training on everything. 
  - Priorities: get a paper out, write up the cloud (tutorial), final coding and show that the code works to impute some whole genome

## More material

- The original iterative testing took about two weeks to sorta converge whereas you sped that up to...
  - ...on the order of hours. It depends on model parameter settings and how much of the genome you are trying to impute. 

- That was operating on 0.06% of the genome and order 100 proteins and order 100 cell types.
  - ChromImpute led to training the model on 127 cell types and 24 assays for comparison purposes 











Q: Have you done the full genome yet? Or if not when do you estimate you'll do that?

See my original message. We hope to still do that, but it won't be all at once; we will train the cell 
type and assay parameters (the smaller dimensions) on a small subset of genomic positions and then apply those 
cell type/assay parameters across the genome in a way that we can work with batches of genomic 
positions instead of all at once. 

When you do or have done the entire genome: You will make your results publicly available to other researchers.

Yes, we will certainly make any imputed data available, and I hope to make my code and a tutorial available 
to anyone who wants to impute their own data. 

- Of the 100 x 100 experiments (I mean protein versus cell type) about how many are actually done?
  - I'm using set of 127 cell types and 24 assays, of which 1014 (33.27%) experiments have experimental data. 
    The full project with more cell types and assays has collected only ~2% of the possible data.




